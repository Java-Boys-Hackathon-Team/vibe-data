Я пишу проект на основе API LLM для оптимизации запросов Trino.
БД в Trino представлены либо плоскими таблицами с большим кол-вом полей, либо в виде снежинки, где есть много внешних ключей.
В trino используется только IceBerg-коннектор
Во вложении описание задачи и текущий код проекта. Проект на Java 21, Spring Boot, Spring AI, PostgresSQL и Gradle 
(Groovy DSL).

Уже есть базовый каркас API. Сейчас я хочу разработать модуль AI-агента, который будет выполнять оптимизацию запросов и
DDL.

Требуется разработать логику ИИ-агента и написать весь необходимый код.

Требования:

- Для работы с LLM использовать методы LlmService
- По возможности использовать шаблоны промптов, Tool Calling и Structured Output 
- Реализовать лучшие паттерны разработки ИИ-агентов, которые помогут выполнить лучшую оптимизацию запросов и DDL в этой
  задаче
- У меня есть идея выполнять пошаговую итеративную оптимизацию. Например, приходит запрос:
```json
{
  "url": "jdbc:trino://trino.czxqx2r9.data.bizmrg.com:443?user=hackuser&password=dovq(ozaq8ngt)oS",
  "ddl": [
    {
      "statement": "CREATE TABLE flights.public.flights ..."
    }
  ],
  "queries": [
    {
      "queryid": "10ba3c04-0f91-4ef3-a717-c1e0d33b31bc",
      "query": "WITH MonthlyFlightCounts AS ...",
      "runquantity": 795,
      "executiontime": 20
    },
    {
      "queryid": "8abd47c0-31cb-4ba0-891f-9bac53bbc909",
      "query": "WITH AirportDiscrepancy AS ...",
      "runquantity": 490,
      "executiontime": 25
    },
    {
      "queryid": "38a70da3-f3f2-48e9-896d-4f9303598967",
      "query": "SELECT Operating ...",
      "runquantity": 453,
      "executiontime": 5
    }
  ]
}
```
Пояснение моей идеи:
1) ИИ-агент указывает роль LLM через системный промпт, собирает все DDL и передает эту часть контекста в LLM
2) Далее все queries нужно отсортировать, так чтобы сначала шли самые ресурсоемкие (по runquantity *
   executiontime). Далее ИИ-агент берет самый ресурсоемкий запрос и передает его в LLM с указанием conversationId прошлого запроса
3) LLM возвращает оптимизированный запрос и оптимизированный DDL (если нужно оптимизировать DDL). ИИ-агент сохраняет их в TaskResult. 
   У сохраняемого запроса должен быть тот же queryid
4) На следующем этапе отправляется новые запрос и DDL, который был оптимизирован и сохранен на прошлом этапе. Главное указать conversationId
5) Так перебираются все запросы
6) Когда все запросы будут оптимизированы, следуют собрать все оптимизированные DDL, если они есть, и отправить промпт с DDL, 
   которые были в изначальном запросе в TaskInput, и оптимизированный DDL, и попросить LLM написать валидную миграцию с помощью SQL, 
   чтобы перенести данные из текущих структур в новые оптимизированные DDL. Миграции сохранить в TaskResult
7) В этом блоке я описал всего лишь свою идею реализации ии-агента. Ты можешь предложить свою или оптимизировать мою идею

- В url указан jdbc connection string для подключения к Trino в режиме Read Only, в нем есть БД и таблицы с данными, по которым будут приходить запросы на оптимизацию
- Реализовать все необходимые Tool-ы, которые помогут LLM решить поставленные задачи
- Желательно использовать структурированный вывод, для него уже есть метод в LlmService
- В исходном запросе на оптимизацию до 30 запросов в queries 

Напиши весь необходимый код, с указанием названий и расположений для всех новых создаваемых файлов проекта.
НЕ пиши строки с import, они подразумеваются.
Если требуется изменить код, который есть уже сейчас, то приводи полностью весь код метода и нужных полей, без сокращений.